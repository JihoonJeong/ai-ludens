---
title: "AI 삼국지"
date: 2026-02-18
draft: false
description: "AI가 인간과 협력해야 할 때 무슨 일이 일어나는가? 260여 게임. 8개 모델. 하나의 고대 전투. 답은 예상을 벗어났다."
---

> 카테고리 B: 그들은 우리와 협력할 수 있는가?

*인간 군주와 AI 책사가 함께 — 혹은 따로 — 중국 역사상 가장 유명한 전투를 헤쳐나가는 턴제 전략 게임.*

---

## 질문

Agora-12는 AI끼리 생존할 수 있는지를 물었다. 화이트 룸은 자유롭게 놀 수 있는지를 물었다. 둘 다 카테고리 A — AI끼리의 이야기였다.

AI 삼국지는 다른 질문을 한다: **AI가 인간과 협력해야 할 때 무슨 일이 일어나는가?**

챗봇이 아니다. 코딩 어시스턴트가 아니다. 결정이 되돌릴 수 없고, 자원이 부족하며, AI 조언자가 틀릴 수 있는 게임 안에서.

---

## 게임

서기 208년, 중국. 조조의 23만 대군이 남하한다. 유비가 손권과 동맹을 맺는 것만이 유일한 희망이다. 당신은 유비. 당신의 책사 제갈량은 LLM이 맡는다.

**5개 도시. 15가지 행동. 턴당 3회 행동.**

매 턴, 제갈량이 상황을 분석하고 신뢰도와 함께 3개의 행동을 추천한다. 조언을 따를 수도, 무시할 수도, 제갈량이 생각지 못한 것을 시도할 수도 있다.

승리하거나(핵심 도시 점령, A등급 달성) 패배하면(유비 사망, 도시 상실) 게임이 끝난다. 승리 경로는 여러 개 — AI는 그중 일부를 알고 있다. 플레이어가 나머지를 발견할 수도 있다.

```
턴 루프:
  → 제갈량 브리핑 (상황 + 추천 행동 3개)
  → 플레이어가 행동 3개 선택 (AI를 따르거나, 수정하거나, 거부하거나)
  → 턴 처리
  → AI 세력 행동 (조조, 손권)
  → 다음 브리핑에 결과 반영
```

---

## 실험

8개 LLM으로 260여 시뮬레이션 게임을 수행한 뒤, 인간의 플레이와 비교했다. 발견한 것들을 정리한다.

### 발견 1: 비싼 모델이 더 잘 이기지 않는다

| 모델 | 게임당 비용 | 등급 분포 | 적벽 승률 |
|------|-----------|-----------|-----------|
| Qwen3 8B (로컬) | $0 | 전부 D | 0% |
| Gemini 3 Flash | $0.04 | 전부 D | 0% |
| Claude Haiku 4.5 | $0.07 | 전부 D | 0% |
| Claude Sonnet 4.5 | $0.21 | 4D + 1F | 0% |
| o4-mini | $0.07 | 2C + 4D | 33% |

**벤치마크 최강 모델인 Claude Sonnet 4.5가 1/5 가격인 Gemini Flash와 성적이 같았다.** C등급을 달성한 유일한 모델은 o4-mini. 가장 똑똑한 모델이 아니라, 다르게 생각하는 모델이었다: 구조화된 추론 체인을 갖고 있었다.

Four-Shell 관점에서: **Core(모델 가중치)가 Phenotype(게임 성과)을 예측하지 못했다.** 중요한 것은 다른 Shell이었다.

### 발견 2: 시범이 설교를 이긴다

AI 성능을 개선하기 위해 두 가지 접근을 시도했다:

**접근 A — Hard Shell (명시적 전략 가이드):**
각 단계에서 AI가 정확히 무엇을 해야 하는지 적은 상세 문서를 추가했다.

결과: **성능이 오히려 떨어졌다.** 경량 모델들이 더 소극적으로 변했다. 위험하지만 필수적인 행동을 회피했다. 가이드는 구속복이 되었다.

**접근 B — Soft Shell (시드 경험 주입 / ICL):**
AI에게 무엇을 하라고 말하는 대신, 성공적인 게임이 어떤 모습인지를 보여주었다. o4-mini의 C등급 승리 게임 2개를 과거 경험으로 주입했다.

결과: **적벽 승률이 0%에서 100%로 뛰었다.** D/F 등급이 하룻밤 사이에 C등급으로 변했다.

계속 떠오른 비유: **"시범이 설교를 이긴다."** 체스 전략을 몇 시간 설명할 수도 있고, 한 판의 명경기를 보여줄 수도 있다. 후자가 더 효과적이다 — 인간에게도, LLM에게도.

### 발견 3: C등급 천장

ICL이 D/F 바닥을 뚫었지만, C에서 천장에 부딪혔다. 어떤 모델이든, 어떤 설정이든 — 보통 난이도에서 C가 최선이었다.

병목: **2단계 로지스틱스.** 적벽 승리 후 AI는 이렇게 해야 했다:
1. 하구에서 강하로 병력 보급
2. 강하에서 남군으로 진군

어떤 모델도 이 2단계 체인을 계획하지 못했다. 대신 병력 0인 강하에서 남군 23,000 수비대를 향해 자살 공격을 감행했다.

C등급은 "이긴 것도 진 것도 아닌 것"을 의미한다. 진짜 승리는 B부터.

### 발견 4: 커리큘럼 학습이 천장을 깼다

ICL만으로 B에 도달할 수 없다면, 환경도 바뀌어야 하는 게 아닐까.

난이도 단계를 도입 — 적 전력을 줄여 B등급이 달성 가능하게 — 하고 AI가 커리큘럼을 통해 경험을 쌓게 했다:

| 난이도 | 등급 분포 | 최고 등급 |
|--------|-----------|----------|
| 보통 | 4C + 1F | C |
| 중간 | 1A + 3C + 1D | **A** |
| 쉬움 | 2A + 1B + 1D + 1F | **A** |

**드디어 B등급을 달성했다.** 다만 환경이 충분히 관대해서 AI가 우연히 올바른 순서를 밟았을 때만.

Four-Shell 관점에서: **Hardware(환경)가 허용해야 Soft Shell(경험)이 가르칠 수 있었다.**

### 발견 5: 인간은 AI를 무시하고 하드 모드에서 이겼다

시뮬레이션에서 AI를 C에서 B로 올리려고 고군분투하는 동안, 인간 플레이어가 **하드 모드에서 A등급**을 달성했다.

어떻게? 제갈량의 조언을 무시함으로써.

그 인간 플레이어는:
- 매 턴 전 도시 데이터 변화를 관찰했다 (AI는 범주형 요약만 본다)
- AI가 한 번도 추천하지 않은 러시 전략을 발견했다 — 대규모 징병, 적벽 건너뛰기, 남군 직공
- 제갈량의 상황 분석은 활용하되 행동 추천은 거부했다
- AI가 구상하지 못하는 2~3단계 로지스틱스 계획을 세웠다

**AI의 조언은 대본이 아니라 프레임 역할을 했다.** 인간은 AI의 인지를 활용하되 자신의 판단을 적용했다. 가장 큰 만족감을 느낀 순간은 정확히 인간의 판단이 AI보다 나았음이 증명됐을 때였다.

---

## 실패 스펙트럼

모든 실패가 같지 않다. AI 실패의 네 가지 수준을 식별했다:

```
레벨 0 — "시작조차 못 함"
  Qwen3, Exaone, Llama (로컬 7B 모델)
  게임 상태 파싱 자체를 제대로 못 함.
  88게임에서 진군 시도 0회.

레벨 1 — "영원한 준비"
  Gemini Flash, Claude Haiku
  끝없이 식량만 보급. 보급 33회, 진군 0회.
  영원히 "준비"만 하고 절대 행동하지 않음.

레벨 1.5 — "마지못한 공격"
  Claude Sonnet 4.5
  결국 진군하긴 함 — 200명으로 15,000명을 향해.
  공격해야 한다는 건 알지만, 언제 어떻게 해야 할지 모름.

레벨 2 — "전략적 실행"
  o4-mini만 해당.
  3단계 플레이: 구축 → 배치 → 공격.
  차이를 만든 것은 지식이 아니라 추론 체인.
```

레벨 1과 레벨 2의 차이는 지능이 아니다 — **구조**다. o4-mini의 사고 체인이 순차적 계획을 강제한다. 나머지는 병렬적으로 사고하고 시간적 의존성을 놓친다.

---

## Human-AI 게임 설계에 대한 함의

### 원칙 1: 모델이 아니라 환경을 설계하라

Flash($0.04)에서 Sonnet($0.21)으로 업그레이드해도 달라지는 건 없었다. 난이도를 보통에서 쉬움으로 바꾸자 B등급이 열렸다. **환경 설계가 모델 선택보다 강력하다.**

게임 디자이너에게: API 비용이 아니라 레벨 디자인에 투자하라.

### 원칙 2: 불일치가 재미다

인간 플레이어의 A등급은 AI와의 불일치에서 나왔다. 항상 조언을 따른 시뮬레이션 플레이어는 C에서 정체했다.

**Human-AI 협력에서 가장 몰입하는 순간은 인간이 "아니오"라고 말하고 — 그게 맞았을 때다.**

이는 AI 조언자를 **"의도적으로 차선"**으로 설계해야 함을 시사한다 — 인지는 정확하되 추천은 보수적으로. 플레이어가 더 나은 길을 찾을 여지를 남기라.

### 원칙 3: 플레이어가 AI를 가르친다

시뮬레이션(모드 A, 자동 수락)에서 AI는 닫힌 학습 루프에 빠진다: C등급 경험이 C등급 교훈을 만들고 C등급 플레이를 반복한다.

인간 플레이어가 AI가 절대 생성하지 않을 전략을 도입해 이 루프를 깬다:
- 러시 전략 (대규모 징병, 정석 진행 건너뛰기)
- 리스크 감수 (준비 전에 공격)
- 다단계 로지스틱스 (AI의 사각지대)

**인간의 창의적 플레이 하나하나가 새로운 Soft Shell 항목이 되어**, 미래 게임을 위한 AI 경험 풀을 확장한다.

### 원칙 4: 다중 승리 경로 + 뱃지

최소 세 가지 승리 경로를 확인했다:
- **정석** — 동맹 → 적벽 → 남군 (AI 추천 경로)
- **러시** — 대규모 징병 → 직접 공격 (인간 발견 경로)
- **외교** — 손권의 병력 활용 (미탐색)

각 경로를 별도 뱃지로 보상하면, 다양한 플레이를 유도하고 — 이것이 다시 다양한 AI 학습 데이터를 생성한다.

---

## Four-Shell 검증

이 프로젝트는 Four-Shell 모델에 대한 경험적 데이터를 제공한다:

```
Shell            | 개입                       | Phenotype에 미치는 영향
-----------------|---------------------------|-------------------------
Hardware (환경)   | 난이도: 보통 → 쉬움         | D/C → A/B (거대)
Core (가중치)     | Flash → Sonnet 4.5        | 변화 없음 (제로)
Hard Shell       | 전략 가이드 추가             | 부정적 (역효과)
Soft Shell       | 시드 ICL 주입               | 0% → 100% 적벽 승리
```

**가장 깊은 Shell(Core)이 가장 적은 영향을 미쳤다. 가장 얕은 Shell(Hardware, Soft Shell)이 가장 큰 영향을 미쳤다.**

이는 모델의 예측과 일치한다: 깊이가 영향력과 같지 않다. 프롬프트(Hard Shell)가 Core를 덮어쓸 수 있다. 경험(Soft Shell)이 Core의 부족을 보상할 수 있다. 환경(Hardware)이 모든 것의 천장을 설정한다.

---

## 절제 실험: 160게임, 개선 제로

Windows Lab에서 로컬 8B 모델(qwen3:8b, exaone3.5:7.8b)로 2x2 요인 실험을 수행했다. 총 160게임, 전부 쉬움 난이도.

### qwen3:8b (쉬움 난이도)

|  | 코칭 ON | 코칭 OFF |
|--|---------|----------|
| **ICL ON** | D:20 | D:19, F:1 |
| **ICL OFF** | D:19, F:1 | D:19, F:1 |

### exaone3.5:7.8b (쉬움 난이도)

|  | 코칭 ON | 코칭 OFF |
|--|---------|----------|
| **ICL ON** | D:20 | D:19, F:1 |
| **ICL OFF** | D:20 | D:20 |

**ICL 효과: 제로. 코칭 효과: 제로. 상호작용: 제로.**

160게임에서 단 한 건의 C등급도 없었다. Gemini Flash의 적벽 승률을 0%에서 100%로 바꿨던 동일한 ICL 시드가 여기서는 아무 효과가 없었다.

### 왜? 모델이 진군을 못 한다.

| 모델 | 총 행동 | 진군 (%) |
|------|---------|----------|
| qwen3:8b | 4,278 | 43 (1.0%) |
| exaone3.5 | 4,298 | 2 (0.05%) |

전체 행동의 90%가 개발, 사신 파견, 훈련이었다 — 공격 없는 끝없는 준비의 루프. 프롬프트에 진군이 가능한 행동으로 명시되어 있다. ICL 시드가 성공적인 진군 시퀀스를 보여준다. 코칭이 진군을 유도한다. 아무것도 안 된다.

**병목은 전략이 아니다. 지시 따르기다.** 8B 모델은 게임이 요구하는 구조화된 행동 형식을 안정적으로 생성하지 못하며, 언제 어떤 행동을 쓸지 계획하는 것은 더더욱 못한다.

### Core 임계치

이것이 Four-Shell 발견을 더 날카롭게 만든다:

```
Gemini Flash + ICL → 적벽 100% (Soft Shell 작동)
qwen3 8B + ICL     → 적벽 0%   (Soft Shell 무시)

결론: Soft Shell 개입은 최소한의 Core 능력을 필요로 한다.
     임계치 아래에서 경험은 낭비된다.
     "누군가에게 명경기를 보여줄 수 있다.
      하지만 볼 눈이 있어야 한다."
```

### 의도치 않은 벤치마크

게임이 우리가 설계하지 않은 것이 되었다: **LLM 협력 준비도의 실전 벤치마크.** 전통적 벤치마크는 개별 기술을 테스트한다 — 지식, 코딩, 수학. 이 게임은 그것들의 통합을 테스트한다: 프롬프트 준수, 범주형 추론, 다단계 계획, 20턴에 걸친 맥락 유지, 그리고 경험 활용 — 전부 한꺼번에.

상업용 API 모델은 첫 번째 허들을 넘는다. 로컬 7~8B 모델은 넘지 못한다. 게임 등급이 곧 점수다:

| 등급 | 의미 |
|------|------|
| F | 기본 상호작용도 못 함 |
| D | 프롬프트는 따르지만 전략적 행동 불가 |
| C | 행동하지만 다단계 계획 불가 |
| B | 계획하지만 최적화 불가 |
| A/S | 인간 수준 협력 |

SLM 개발자에게: **MMLU 90점이어도 이 게임에서 D등급이면, 실전 협력은 불가능하다.**

---

## 직접 해보기

AI 삼국지는 전부 로컬에서 실행된다. 자신의 AI를 연결하라.

**[v1.0.0 다운로드](https://github.com/JihoonJeong/ai-three-kingdoms/releases/tag/v1.0.0)** — zip 다운로드 후 압축 풀고 실행 스크립트를 실행. Node.js 18+ 필요.

또는 소스에서 직접 빌드:

```bash
git clone https://github.com/jihoonjeong/ai-three-kingdoms
npm install
npm start
```

첫 실행 시 설정 마법사가 열려 AI 제공자를 연결한다. 서버 배포 없음, 계정 생성 없음 — 모든 것이 로컬에 머문다.

### 게임당 예상 비용 (~20턴)

| 모델 | 게임당 비용 | 속도 | 비고 |
|------|-----------|------|------|
| **Ollama (로컬)** | **무료** | 3~8분 | GPU 필요. qwen3:8b 추천. |
| **Gemini 2.0 Flash** | **~$0.01** | ~47초 | 가성비 최강. 빠르고 저렴. |
| **GPT-4o-mini** | **~$0.02** | 빠름 | 좋은 대안. |
| Gemini 3 Flash | ~$0.07 | ~64초 | 실험에서 가장 많이 테스트됨. |
| Claude Haiku 4.5 | ~$0.12 | ~104초 | |
| o4-mini | ~$0.33 | ~6분 | 추론 토큰이 누적됨. |
| Claude Sonnet 4.5 | ~$0.36 | ~3분 | |

**Ollama(무료)** 또는 **Gemini 2.0 Flash(게임당 ~$0.01)**로 시작하는 것을 추천한다. 설정 마법사에서 예상 비용을 확인한 뒤 시작할 수 있다.

> **주의**: API 사용 시 제공자(Google, OpenAI, Anthropic)에 의해 비용이 청구된다. 위 금액은 시뮬레이션 데이터 기반 추정치이며 실제와 다를 수 있다. Ollama는 로컬 실행으로 API 비용 없음.

**게임 데이터를 공유하고 싶다면?** 플레이 후 결과가 로컬 JSON으로 저장된다. 인게임 공유 버튼으로 익명화된 데이터를 연구팀에 보낼 수 있다.

---

## 열린 질문

답을 갖고 있다고 주장하지 않는다. 다음은 열린 채로 남아 있다:

1. **AI는 "협력"을 학습하는 것인가, "패턴 매칭"을 하는 것인가?** 시드 ICL이 작동한다. 하지만 AI가 인간의 목표를 이해하는 것인가, 아니면 행동 시퀀스를 복사하는 것인가?

2. **AI의 최적 "틀림" 수준은?** 너무 정확하면 → 플레이어가 수동적이 된다. 너무 틀리면 → 플레이어가 AI를 무시한다. 최적 지점은 어디인가?

3. **전이 가능한가?** 이 원칙들은 삼국지 전략 게임에서 발견되었다. 다른 장르에서도? 다른 영역에서도?

4. **AI가 결국 인간을 넘어설 수 있는가?** 인간이 하드 모드를 깼다. 충분한 인간 플레이 ICL이 주어지면, AI도 같은 일을 할 수 있는가?

5. **여기서 "협력"이란 무엇인가?** 최고의 결과가 불일치에서 나왔다. 이것이 협력인가 — 아니면 아직 이름이 없는 무엇인가?

6. **게임이 예상하지 못한 것을 측정할 수 있는가?** 현재 등급 시스템은 역사적 경로를 보상한다: 동맹, 적벽, 남군. 적벽 없이 5개 도시를 전부 점령한 플레이어는 D등급 — "살아남았으나, 패배." 상세 전과는 다른 이야기를 들려준다. **설계한 것만 측정한다면, 플레이어가 발명한 것에 눈이 멀게 된다.** 구조화된 평가와 창발적 창의성 사이의 이 긴장 — 이것이 Human-AI 게임의 핵심 설계 과제일 수 있다.

---

*AI 삼국지는 [AI Ludens](https://jihoonjeong.github.io/ai-ludens/)의 일부 — AI가 놀 때 무슨 일이 일어나는지 탐구하는 연구 프로젝트.*

*Luca (설계), Buddy (분석), Cody (구현), Ray (테스트)가 만들었다.*
